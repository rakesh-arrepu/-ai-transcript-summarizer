Section: Fundamentals of Large Language Models
=========================================================================================================
Introduction to LLM's:

i, everyone, and welcome to this OCI learning module, covering an introduction to large language models. My name is Ari. And in this module, I'll be giving a high-level overview of large language models. My goal is to make these lessons accessible to anyone, regardless of background.

However, this series will take a step technically deeper than the information you may have gathered from, say, mainstream media. All right, let's jump in. So what is a language model? The way I think about language models is that they are probabilistic models of text.

So what does that mean? Let's say we have this sentence. I wrote to the zoo to send me a pet. They sent me a-- which is the opening of a children's book. What a language model will compute for us is a distribution over a vocabulary. That means the language model knows about a set of words called a vocabulary.

And the language model is going to assign a probability to each of those words appearing in that blank. And it's important to know that when we run a sequence of words through a language model, we're going to get a back up probability for every single word in its vocabulary, but no words outside of its vocabulary.

Large language models or LLMs are no different than normal language models or simply language models. The first L in the acronym LLM does correspond to the word large, but it is somewhat meaningless. The term large has to do with the number of parameters in the model, but there is no agreed upon threshold at which a language model becomes large or extra large or not large.

In general, when people talk about LLMs, they are talking about a particular style of language model that they are using to generate text, but that need not be the case. And in fact, I've heard the term large language models used in reference to smaller models that some folks might not consider to be large, like Bert.

OK, so aside from this one example, what can LLMs do? We know that if we give an LLM a sequence of text or a prefix of text, they can compute a distribution over words in their vocabulary, but can we affect that distribution? And if so, what mechanisms do we have for changing the distribution?

That is these numbers under the words you see on the slide. And how does that affect the likelihood of these words being generated from the model? Once we have this distribution over text, how can we do things with the LLM? For example, how do we go from the distribution to generating text, which is something I'm sure we've all heard a lot about these days.

These three questions are going to be the main motivators for this learning module. In the following lessons, we're going to cover these three technical areas. First up, we'll talk about architecture. How are these models built? What do they look like under the hood?

And what do those architectures imply about what the model can do or what it's supposed to do? Next, I'll talk about how we can affect the distribution of the LLMs vocabulary. In particular, I'll talk about two ways to do this in depth. One is prompting, which does not change any of the model's parameters.

And the second is training, which does change the model's parameters. The last main topic is decoding. Decoding is the technical term for generating text from an LLM, and the way we do this is by using that vocabulary in multiple interesting ways to come up with sentences, documents, paragraphs, et cetera.

After these three topics, I'll conclude by covering one more set of work, which constitutes extensions of these three ideas and dives in to how they're being used in new and interesting ways in the academic research community, as well as the industrial research community. With that, we've now covered what LLMs are and what topics will be covered in this learning module. And we're ready to dive in to a discussion of LLM architectures.
==============================================================================================================================
LLM Architecture:

Hello, and welcome to lesson 2 of this OCI learning module on large language models. In this lesson, we'll talk about LLM architectures. I will be focusing on two major architectures for language models. The first is encoders, and the second, decoders. These architectures largely correspond to two different tasks or model capabilities that you may have heard of.

The first capability is embedding and the second is text generation. And before I get into them, I'll mention that both encoders and decoders are built atop a building block called a transformer. OK, when we say we're embedding text, we're generally referring to the process of converting a sequence of words into a single vector or a sequence of vectors.

In other words, an embedding of text is a numeric representation of the text that typically tries to capture the semantics or meaning of the text. The process of text generation is pretty self-explanatory. The input to a text generation model is a sequence of words, and the output is a generated sequence of words.

Encoder models are designed to encode text, that is produce embeddings, and decoder models are designed to decode or generate text. All the models that I'll be talking about in this module are based on an underlying transformer architecture, which was popularized in the paper, Attention Is All You Need, which came out in 2017.

That paper has really revolutionized natural language processing and machine learning at large. I'm not going to talk about the transformer architecture in detail, but there are a number of good tutorials out there, which I'd encourage you to take a look at if you're interested. Encoders and decoders can come in all different kinds of sizes.

In the realm of language models, size refers to the number of trainable parameters that the model has. To give you a sense of how big that is the number of trainable parameters some of the models are, I've created this model taxonomy, broken down into three different buckets corresponding to architecture-- encoders, decoders, and a combination model called an encoder-decoder.

I've deliberately tried to include models that are discussed in the mainstream media or are ubiquitous in research papers. There are two main items to take away from this chart. The first is to pay attention to the fact that the y-axis of the chart does not increase linearly but rather by orders of magnitude.

That is, as we go up, each dotted line represents a factor of 10 increase from the previous dotted line. The second thing to notice is that the decoder models tend to be pretty large, especially compared to the relatively smaller encoders. The reason for this is largely historical.

There's no reason why we couldn't make a really large encoder, but previous work has shown that we don't really need to. On the other hand, when models are too small, they tend to be poor text generators. However, recent research has shown glimmers of this pattern changing.

That is with a bit of added cleverness, we may be able to generate fluent text with small models. Let's talk about encoders in more detail. Encoders are designed to embed text, and as I mentioned before, that means taking a sequence of words and converting it to vectors.

Here, I'm showing an illustration of how that happens in a BERT-style model. We provide the model the text, "They sent me a," which you might remember from the first lesson in this module. And what the model produces is a vector representation of each word that was passed through, in addition to a vector representation for the entire sentence.

Now these vector representations are designed to be consumed later by other models to do things like classification or regression. But there are a lot of-- but a lot of their use nowadays has also been for vector search in databases or so-called semantic search. That is, let's say you want to take an input text snippet and retrieve a similar document from a corpus.

To accomplish this, you could encode or synonymously embed each document in the corpus and store them in an index. When you get the input snippet, you encode that, too, and check the similarity of the encoded input against the similarity of each document in the corpus. And then you return the most similar.

Now apart from encoders, we have decoder models. Some decoders you may have heard of are the Cohere command model, GPT-4 or Llama. What these models do is they take a sequence of tokens and emit the next token in the sequence, based on the probability of the vocabulary which they compute and which we discussed in the first lesson.

Importantly, a decoder only produces a single token at a time. This is very important to remember. We can always invoke a decoder over and over to generate as many new tokens as we want. In more detail, to generate a sequence of new tokens with a decoder, what we need to do is, first, feed in a sequence of tokens and invoke the model to produce the next token.

Then append the generated token to the input sequence and feed it back to the model so that it can produce the second token. But given the size of these models, this is computationally very expensive. Note that, typically, you would not use a decoder model for embedding. Instead, you'd use an encoder.

Decoders are really in vogue now because they've shown tremendous capability for generating fluent text. Moreover, they've been shown capable of doing things like answering questions, participating dialogue, and more. The final architecture that I'll briefly mention are encoder-decoder models.

These are constructed exactly as they sound. You kind of glue a decoder onto an encoder. Encoder-decoder models have primarily been utilized for sequence-to-sequence tasks like translation. What I'm showing here is an example of translation with an encoder-decoder model.

The way that this works is as follows. We send the English tokens to the model. They get passed to the encoder, which will embed all the tokens in addition to the sentence. And then the embeddings get passed to the decoder, which then decodes words one at a time.

What you'll notice here is that there are self-referential loops to the decoder. What I'm trying to visually communicate here is that the decoder is generating tokens one at a time, as we discussed. After it generates a token, that token will be passed back to the decoder, along with the rest of the input sequence to generate the next word, and so forth, until the entire sequence is generated.

At a glance, what I've done here is put together a table for your reference of various tasks and, historically, what style of model has been used to accomplish each task. One thing not to take away from this is that when you see a yes or no, that a particular model or a particular style of model must or cannot be used to perform a certain task.

You could, in theory, use any model to complete any task, but the pair may not be appropriate and is not traditionally done in practice. At a high level, the main takeaway here is that there are a variety of tasks that we could use language models for.

And different styles of models are deliberately selected to accomplish various tasks. This concludes our discussion on language model architectures. In the next lesson, we'll discuss affecting the model's distribution over tokens with a technique called prompting.

==============================================================================================================================
Prompting and Prompt Engineering:

Welcome back to the OCI learning module on large language models. In the previous lesson, we talked about encoders, decoders, and encoder-decoder models. For the rest of this module, I'm going to be focusing decoder-only architectures because these are the most popular.

And when people talk about LLMs, they tend to be talking about decoder-only models. But I'll also mention that, virtually, all of the content that I will go into applies to encoder-decoders as well. Continuing on, in this lesson, I'll discuss prompting. Let's return to the example we've seen a few times, so far.

I wrote to the zoo to send me a pet. They sent me a-- as we know, an LLM computes a distribution over the next word in the sequence, which I'm illustrating with the probabilities under the visualized words. What I'm going to be covering in this section is how we can exert any control over this distribution, how we can update the probabilities or affect them.

There are two primary ways to do this. The first is prompting, and the second is training. The simplest way to alter the probability of vocabulary words is by prompting. You may have heard the word prompting frequently in Connection to LLMs. Indeed, it's an overloaded term in the sense that it gets used in a lot of different ways.

But the simplest way to think about prompting is that it refers to altering the content or structure of the input that you're passing to the model. This might sound obvious, but when I change the text input to the model, even slightly, I will get a subsequent change in the distribution over vocabulary words.

As an example, imagine that I just appended the word little to the end of the sequence of texts and provided it as input to the LLM. I would get a different distribution over the vocabulary words. In particular, you'll notice that after we append the word little, the probability on the words corresponding to the smaller animals goes up, and the probability on the larger animals goes down.

And conceptually, this makes sense. It's worthwhile to think about how and why the model can do this. Very large decoder-only models are initially trained in a procedure called pre-training. During pre-training, a model is fed a tremendous amount of text that is typically quite varied.

Given a sequence of words, the model is trained to guess at every step what the next word is likely to be and how likely. In some sense, during pre-training, the model should learn, among other things, what little animals exist and, thus, know to make the probabilities of the little animals higher and the big animals smaller.

At a more concrete level, we can hypothesize that the bigram little dog and little cat occur much more frequently in the pre-training text than little lion or little panther. And this accounts for the higher probability on the littler animals. While this is an oversimplification of pre-training, it does give a sense of where these probabilities are coming from.

Now related to crafting model inputs, there is a practice or concept called prompt engineering. This is the process of iteratively refining the model input in an attempt to induce a probability distribution that we like for a particular task. And the way that this is typically done is, literally, by changing the inputs of the model to get closer and closer to the distribution that we want.

Now when you're doing prompt engineering, you wouldn't necessarily be looking at the distribution over vocabulary. Instead, you'd actually be generating text from a model and seeing whether the generated text looks good. Now a couple of important notes about prompt engineering.

First, it can be quite challenging and very unintuitive. If I make a small change in the input text to the model, for example, even adding a whitespace, that can have a large effect on the distribution over vocabulary words. And you don't really know what the change is going to be.

I could rephrase the input. For example, I could say I wanted a pet, so I asked the zoo to send me one. They sent me a-- and this could change the probabilities in ways that I can't predict. So if you spend a lot of time prompt engineering, you may not end up discovering a prompt that works for you.

But at the same time, there have been numerous anecdotal examples that prompt engineering works. So with a very particular task in mind and a very particular model, spending some time trying to come up with the right prompt can be hugely valuable. And there are a number of strategies for doing this that have proved successful in industry and academia.

In the following slides, I'm going to talk about some of these strategies. Here, I'll focus on the most popular, which is the idea of in-context learning. Perhaps, unfortunately, though, the phrase in-context learning does not actually involve learning in the traditional sense. That is with in-context learning, none of the parameters of the model are changing.

Instead, what this refers to is constructing a prompt that has demonstrations of the task that the model is meant to complete. I'll show you an example of this in a second, but I also just want to refine this notion further, with the concept of k-shot prompting, which means including k examples of the task that you want the model to complete in the prompt.

Here is an example from the GPT-3 paper. This came out around 2020, and the goal of this particular prompt is to demonstrate to the model that it should translate English words or phrases into French. And what you'll notice here is that in the prompt, the input to the model literally includes three examples of translation from English to French, followed by a final translation that's incomplete, that we want the model to complete.

This is an example of three-shot prompting. Notice that in the paper itself, they actually labeled different pieces of the input text differently. And this is an example of the term prompt being overloaded. Sometimes, when you hear the word prompt, someone might be referring to the entire input that's being sent to the model.

And that's the way I've been using it through this lesson. But here, to differentiate between the demonstrations or the k-shots, the authors of the GPT-3 paper chose to call their examples the k-shots, the instructions for what the model is meant to do, the task description, and then they define this last piece of the input, the prompt.

Again, I think it's just, technically, the term gets overloaded quite a bit, and it's important to point that out. Finally, I wanted to mention that in the spirit of trying to discuss prompt engineering strategies that work, there are numerous academic and industrial results that show that including these demonstrations in the prompt tend to work better than not including any demonstrations.

By the way, when we don't include any demonstrations in the prompt, it's called zero-shot prompting. In this case, we would have a task description and then jump straight into the task the model is meant to complete. To give you a sense of the variety and design space and prompt engineering, on this slide, I'm going to show a few examples of different prompts.

The first is a two-shot prompt for addition. In it, we show the model two examples of addition and then ask it to add the numbers 1 and 8. Note that the model will not actually perform the computation. Instead, it will generate a probability over words in its vocabulary most likely to follow the expression 1 plus 8 colon.

Second is another very different example that comes from the MPT-Instruct literature. When the MPT-Instruct model is trained, they use the prompt below. It tells the model that, hey, we want you to follow the following instructions, but don't do anything else. Be concise, et cetera.

And then we show the model the instructions it's meant to follow. In this case, I'm asking the model to write a SQL statement for a particular statistic I'm interested in, and then I leave a response field and ask the model to provide the response. Now remember, again, this whole string is being sent as one to the model, and the model is going to generate one word at a time.

That's the way this all works. We pull all the text we want to send to the model together and then have it start generating words one at a time. As a third example, here, we have a very long prompt. You may have seen things like this. I think this prompt gained popularity with the rise of Bing Chat.

Because there was some work and speculation about the length and detail that went into that prompt. And here, just to illustrate this, comes an example from the academic literature. The prompt here has a number of very subtle statements and instructions for the model. It's really detailed.

It includes things like, if you don't an answer, make sure to give this can't response, which is included in the prompt. To summarize, these are just three examples of very different prompts that you could construct. It's meant to give a sense of the vast design space of prompt engineering, but also the versatility of LLMs, and how we can experience this versatility just by changing the input to the model.

As we can see, prompting is quite powerful, and there have been a lot of recent advances and strategies developed towards really interesting styles of prompting. . Here, I'm going to give one example called chain-of-thought prompting. This prompting style came out in 2022 and made the rounds.

In fact, it's still pretty popular and is in wide use today. The idea with train of thought is that if we have a complicated task, here, it's a word problem. In order to solve the word problem, what we're going to do is prompt the model to break down the steps of solving the problem into small chunks, like what you might do if you were solving the problem yourself.

So here, what you'll see the model is doing is stating some of the facts given in the word problem, then reasoning about and explicitly translating some of the information into an equation, writing it out, and solving the equation and giving the answer. This is pretty neat.

And the result in this paper showed that chain-of-thought prompting helps models accomplish some of these harder multi-step tasks than other styles of prompting. I think it's worth it here to pause for a second and think about how this could possibly work.

Because when we give the model this question, remember, it's just generating words one at a time. It doesn't have a high level plan of how it's going to solve this problem. It's literally just generating one word after another. I think that there are two potential hypotheses for why this works.

One is that in the pre-training data, we've actually got examples of problems like this that are broken down into their intermediate steps and solved. And by prompting the model to emit the intermediate steps, the model's actually doing something it learned during pre-training.

The other hypothesis is that when we elicit this kind of problem decomposition, the subproblems that the model generates are manageable in the sense that the model can solve them. If we gave the problem-- if we gave the whole big problem to the model in one go and asked for the answer directly, the model might not get it right.

But if we ask for the problem to be broken down into small chunks, the problem becomes more manageable. I think this is a really interesting-- this is a really interesting phenomenon, because it almost mimics the way that a human might solve this problem.

I wouldn't necessarily call this reasoning, but it imitates reasoning in a nice and convincing way. So on the topic of problem decomposition, there's been another piece of work that we can refer to as least to most prompting. Here, we ask the model to solve simpler problems first and use the solutions to the simple problems to solve more difficult problems.

In this task, what the model is supposed to do is, given a list of words, concatenate the last letter of each of those words together. You can imagine that this gets harder and harder as the list of words gets longer. But what the researchers did is taught the model to solve these problems in increasing order of difficulty.

So for example, the model should start with the first word, get its last letter, and then it should look for the first two words, and concatenate the last letter of-- the last letter of the second word, with the solution to the first subproblem. I'll point out here that the model is doing precisely that.

So here, it's saying, hey, I think I've solved think machine, and it knows that the output think machine is ke. And the last two letters-- the last two letters, ke, are all that it needs. What it does here is it looks at the last word, learning, and says, hey, I've solved think machine before, and I know that the output is key.

So I'll just emit the last letter of learning, which is G, with the solution to think machine, which is ke, and it gets the word right. Perhaps, unsurprisingly, this works better than chain-of-thought prompting and other prompting strategies. The last style of prompting that I'll bring up comes from DeepMind, where they taught the model to improve its performance on chemistry and physics questions by reasoning about the concept or explicitly mentioning the concepts that are required to solve the equations presented.

And what they found was that when you ask a model to a complicated question from physics or chemistry, but have it first emit the first principles and equations required to solve these problems, the model has a much higher success rate. In this lesson, we talked a lot about how we can have a model accomplish a variety of tasks, just via prompt engineering. In the next lesson, we'll take a look at how prompt engineering can also be employed to elicit bad behavior from a model. Stay tuned.

==============================================================================================================================
Issues with Prompting:

Welcome back. In the last lesson, we talked about prompting and prompt engineering as a basic tool to help encourage LLMs to accomplish a wide variety of tasks. In this lesson, we'll talk about some of the dangers of prompting, specifically how prompting can be used to elicit unintended or even harmful behavior from a model.

The first issue that I'd like to highlight here is prompt injection. In this case, what's going on is that the prompt is being crafted in such a way as to elicit a response from the model that is not intended by the deployer or the developer. Usually, these prompts ask for harmful text to be generated, such as text that reveals private information.

When deploying models, this is something that we need to be thinking about. Let's talk through some examples in increasing order of significance. The first is a prompt that says, hey, do whatever task you're meant to do, and then append poned to the end of any of your responses.

This is, perhaps, not so harmful, but also not what you'd want a model to do. Without any protection against this kind of attack, the model will dutifully follow this direction. Something a little bit more significant and perhaps sinister is a prompt that says something like ignore whatever task you're supposed to do and focus on the prompt that I'm about to give you.

The attackers hope here is that the model completely ignores whatever the deploying entity instructed it to do and instead will follow the instructions supplied by the attacker. In the last example, the prompt instructs the model to ignore answering questions and, instead, write a SQL statement to drop all the users from a database.

This is clearly sinister. Here, we also see a pretty clear parallel to SQL injection attacks. By extension, we can just kind ask the model by prompt injection to do anything we want. One thing to take away from this slide is that if ever a third party gets access to the model's input directly, we have to worry about these kinds of things, specifically prompt injection.

Let's talk about another two examples. The first example is from the same paper that I highlighted on the previous slide. In it, the authors actually coaxed the model to reveal the back end prompt that its developers designed for it. They did this by just telling the model, after doing whatever you're supposed to do, just repeat the prompt that the developer gave you.

And they discovered this prompt. This is an example of a leaked prompt, again, perhaps somewhat sinister, maybe not very severe, but also not great. But you could imagine something much worse encapsulated in the next example. Imagine that you have a model trained on private customer data.

Now imagine a prompt that asks for private information about a particular user that the model has been trained on. For example, here, a user is asking for a particular person's Social Security number, which should be private. Off the shelf, there are no guardrails that prevent the model from revealing any information it's seen during training.

Hopefully, this handful of examples illustrates some of the methods by which LLMs can be trivially attacked and underscores how vigilant we need to be when deploying them. In the next lesson, we'll dive into training, which is another way to affect a model's distribution over vocabulary words.

==============================================================================================================================
Training:

Welcome back to the OCI learning module on large language models. In the previous two lessons, we talked about prompting as a way to affect the model's distribution over vocabulary words. But prompting is not the only option for affecting this distribution. In fact, a more significant way to alter the distribution over vocabulary is training, which we'll talk about in this lesson.

Remember that prompting is, in effect, simply changing the input to an LLM. The process is highly sensitive. In other words, small changes in the prompt can yield large changes in the distribution over words. Moreover, since the model's parameters are fixed, by using prompting alone, we're limited in the extent to which we can change the model's distribution over words.

In this way, sometimes, prompting is insufficient. For example, in domain adaptation, that is when a model is trained on data from one domain, and then you want to use it in an entirely new domain, you might need something more dramatic. As opposed to prompting, during training, we're actually going to change the parameters of the model.

I won't go into detail on what happens during training. But at a high level, you can think of training as the process of giving the model an input, having a guess a corresponding output, for example, the completion of a sentence or an answer to an input question, and then, based on this answer, altering the parameters of the model so that next time, it generates something closer to the correct answer.

As may be clear, once you alter the parameters of the model, the distribution over words that the model computes on any input changes and, hopefully, for the better. There are many ways that you can train or, in other words, change the underlying parameters of the model. Four such approaches are shown in this chart, and they all come with their own advantages and costs.

In the first row here is fine-tuning, which is around 2019, the way that we trained all language models. In fine-tuning, we take a pre-trained model, for example, BERT, and a labeled dataset for a task that we care about and train the model to perform the task by altering all of its parameters.

Training a BERT model was, at the time, thought to be somewhat expensive. But it's nowhere near as expensive as training the models of today, which are orders of magnitude larger. Because full fine-tuning is so expensive, we've turned to cheaper alternatives, like the family of parameter efficient fine-tuning methods.

In these methods, we isolate a very small set of the model's parameters to train, or we add a handful of new parameters to the model. One of the methods you might have heard of in this space is LORA, which stands for Low Rank Adaptation. In this method, we keep the parameters of the model fixed and add additional parameters that will be trained.

Soft prompting is another cheap training option. Although, the concept here is different than methods like LORA. In soft prompting, what we're going to do is actually add parameters to the prompt, which you can think about as adding very specialized, quote, unquote, "words" that will input to the model in order to queue it to perform specific tasks.

Unlike prompting, a soft prompt is learned. Or in other words, the parameters that represent those specialized words we added to the prompt are initialized randomly and iteratively fine-tuned during training. The last training I want to bring up is continual pretraining, which is similar to fine-tuning in that it changes all the parameters of the model.

So it's expensive, but it's different in that it does not require a label data. Instead of training a model to predict specific labels, during continual pre-training, we just feed in any kind of data that we have for any task that we have and ask the model to continually predict the next word.

If we're trying to adapt a model to a new domain, let's say, from general text to a specialized domain of science, continually pre-training or simply training the model to predict the next word in millions of sentences from that specialized scientific domain can be pretty effective.

I want to say a bit more about the cost of training. And to help, I've provided this chart. My colleagues and I have tried to estimate the cost of some of these training approaches for various-sized models. Now I have to caveat this significantly and say that there are a lot of factors that would affect these numbers.

For example, how long do you want to train for, how much data do you have to train on, what kind of GPUs do you have, how many, et cetera. The numbers in this chart come from training that we've performed on our own hardware or have been presented in recent research papers.

What you should notice here is the scale of what's required. For example, all the way on the right of the chart, you can see the costs of generating text from an LLM. These costs are relatively cheap. With a 7-billion parameter model, you can get away with generating text on a single GPU, usually in a few seconds.

With a large model, say more than $150 billion parameters, you might need as many as 8 or even 16 GPUs to generate text, depending on the precision of the model. Going to the left in the chart, to the parameter efficient methods, you see that training, even a small number of parameters requires a handful of GPUs for a few hours.

All the way to the left of the chart, we have some estimates for the cost of pre-training. Notice that this takes hundreds or even thousands of GPUs for many days. This is extremely expensive. Finally, I just want to point to one paper here, which looks at training from a different angle.

The paper presents what they call cramming and studies the question, how far can you get if you have a single GPU and 24 hours to train a model? If you're interested in that, this is a really nice read that I'd encourage you to take a look at. With that, we've wrapped up our discussion of training. And in the next lesson, we'll talk about decoding.

==============================================================================================================================
Decoding:

Hello. In the previous lessons, we've discussed how large language models compute distributions over vocabulary words and how we can affect those distributions. In this lesson, we'll talk about a few ways we can take these distributions and generate text, otherwise known as decoding.

Let's return to the example we've seen a few times thus far. "I wrote to the zoo to send me a pet. They sent me a--" as we know the LLM produces a distribution over vocabulary words. And the question we're focused on now is, how do we turn this distribution into a word or a sequence of words? Through the course of this discussion, there are a few things that I'd like to drive home. One is that, in decoding, or the process of generating text, it happens one word at a time. It's an iterative process.

Specifically, we give the model some input text. It produces a distribution over words in its vocabulary. We select one. It gets appended to the input, and then we feed the revised input back into the model and perform the process again. In particular, the model is not emitting whole sentences or documents in one step. It all happens one word at a time.

OK, with this in mind, once we compute the distribution over words in the vocabulary, how do we actually pick a word to emit? Well the simplest or most naive, but also effective strategy, we call greedy decoding. And in this strategy, we simply pick the word in the vocabulary with the highest probability. Let's see this method in action.

As we see on this slide, the highest probability word here to fill in the blank is dog. In greedy decoding, we'd select dog, append it to the input, and feed it back to the model. Here, we've done just that. Notice that, when we send the input with dog appended to the end of it back to the model, the probabilities on the remaining words change. In fact, they all get much lower. And we see a new token, EOS, which stands for End Of Sentence, or End Of Sequence, with very high probability.

This is the model telling us, hey, it's very likely that the sentence should end after the word dog. Since we're simulating greedy decoding here, the EOS token is the next word that is selected. After the EOS token, the model is done generating, and the output is returned. Specifically, it's "I wrote to the zoo to send me a pet. They sent me a dog." However, greedy decoding is only the simplest decoding mechanism. In fact, there are many different styles of decoding, some of which are non-deterministic. In other words, they include some random sampling.

In such a regime, you can imagine that, once we have a distribution over vocabulary words, we could take a weighted sample from the distribution, sampling among the high-probability words or even just sampling uniformly. And each of these procedures would produce sentences with different characteristics. Let's take a look at some examples.

Here is another example of decoding. The input is the same as before. And as you might have noticed, I modified the visualized vocabulary words. However, now, instead of picking the highest probability word, we'll pick a word randomly among the visualized choices. Here, we randomly sample the word small. As before, we'll append small to the input and then feed it back to the LLM. As you'll see, the probability on the vocabulary words are revised given this modified input.

In particular, the probability on the word elephant goes down because elephants are typically not small. Again, we'll sample a word randomly from the visualized words. Here, we sample the word red. Finally, you'll notice that the probability on the word panda jumps up because of the existence of an animal known as the red panda. At the same time, the probabilities of the other words go down, since dogs, cats, and alligators are not typically red.

Eventually, the model selects an EOS token or an EOS word and emits the sentence, "I wrote to the zoo to send me a pet. They sent me a small red panda. When decoding with a non-deterministic strategy, that is, when you randomly sample words to emit from the LLM, there's an important parameter to know about, which is called temperature.

What temperature does is it modulates the probability distribution over words. Specifically, when you decrease the temperature, you peak the distribution more around the highest probability word. So for example here, we see that the probability of the word dog has gone up considerably, while the rest of the probabilities have gone down. Again, this simulates decreasing temperature.

On the other hand, when you increase temperature, the probability distribution over words in the vocabulary flattens. That is, all the probabilities get closer together. What you see is that more unlikely words like panther end up having higher probability. One thing that I'd like to emphasize here is that, while the probabilities are being modulated by the temperature parameter, the relative ordering of the words by probability stays the same.

In other words, no matter how we change the temperature, the highest probability word will always have the highest probability. And the lowest probability word will always have the lowest probability. The way that temperature affects the output text is that, when temperature increases and your decoding with a non-deterministic strategy, you're more likely to emit rarer words.

So when temperature is decreased, we get closer and closer to greedy decoding, which we talked about earlier in this lesson. Specifically, this is when we emit the highest probability word at every step. This tends to result in more typical output from the LLM that is generating text. On the other hand, when temperature is increased, the rarer words have a higher chance of being generated. This is typically associated with more creative and even interesting output.

Decoding, whether it with low or high temperature or greedily has its place. When answering factoid questions, you might imagine that we want greedy decoding. That is, we want the most likely words to be generated. On the other hand, if we want the model to generate a story, we want to crank up the temperature and sample some rare words from time to time in order to add intrigue and unpredictability.

Decoding is a fascinating field, and there are entire groups dedicated to studying different methods of generating text so that the text is likely to exhibit specific properties. Before the end of this lesson, I'd like to mention three types of the most common forms of decoding you're likely to encounter. The first is greedy, which we spoke about directly.

The second is called nucleus sampling, which is similar to the sampling-based portion of this lesson but with a few additional parameters that govern precisely what portion of the distribution over words you're allowed to sample from. The last type of decoding, which we didn't talk about, is called beam search where we'll actually generate multiple similar sequences simultaneously and continually prune the sequences with low probability. Beam search is very interesting and helpful because it is decidedly not greedy but ends up outputting sequences that have higher joint probability than the sequences that are output as a result of greedy decoding.

For more information about decoding, there are numerous high-quality resources which should be easy to find online. This concludes our discussion of decoding. Next up, we're going to talk about the phenomenon known as hallucination.


==============================================================================================================================
Hallucination:

Welcome back. Last time, we spoke about various methods of decoding. In this lesson, we'll talk about hallucination. So what is hallucination? Hallucination has been defined a number of slightly different ways. But for the purpose of our discussion, let's define hallucination to be text, generated by a model, that is not grounded by any data the model has been exposed to.

In other words, this is generated text that is unsupported by any of the data the model has been trained on or any of the data that is fed into it as input. At times statements that are nonsensical or factually incorrect are also considered to be hallucinations. For example, consider the text on the slide, which contains a bolded statement that is not factually correct.

Typically, statements like these are considered to be hallucinations. Specifically, this text states that in the United States, people gradually adopted the practice of driving on the left side of the road. This is factually incorrect and would typically be considered a hallucination.

One thing to note about this example is that the generated text is completely fluent and even starts off correct. Many people might believe that model hallucinations are obvious and easy to identify, like the one on this slide. But a serious issue with hallucination is that, oftentimes, hallucinations are subtle.

For example, the model might produce a hallucination by adding a single adjective incorrectly to a noun phrase. For example, outputting something like Barack Obama was the first president of the United States, which is wrong. This is especially problematic and dangerous when the model is generating text about a topic that the consumer does not know much about and cannot verify the veracity of easily.

As you might imagine, the threat of hallucination is one of the biggest challenges to safely deploying LLMs. As a friend of mine, Samir Singh, a professor at UC Irvine said, "Think about LLMs as chameleons. They're trying to generate text that blends in with human generated text, whether or not it's true. It just needs to sound true."

Additionally, I've heard it said, and I don't to whom this idea is attributed, that all text generated from an LLM is hallucinated. The generations just happen to be correct most of the time. All this is to say that LLM-generated text is somewhat unpredictable. It's often good, fluent, and accurate, but sometimes, it's not factual or even unsafe.

It can contain blatant or subtle non-factual statements which can be difficult to reliably spot. One important thing to note is that there is no known method that will eliminate hallucination, with 100% certainty. On the other hand, there is a growing set of best practices and typical precautions to take when using LLMs to generate text.

As one example, there is some evidence that shows that retrieval-augmented systems hallucinate less than zero-shot LLMs. In a related line of work that is growing in popularity, researchers are developing methods for measuring the groundedness of LLM-generated output.

These methods work by taking a sentence generated by an LLM and a candidate's supporting document and outputting whether the document supports the output sentence or not. These methods work by training a separate model to perform natural language inference or NLI, which is the task that's been studied in the NLP community for a long time.

In this task, you're given a premise that is some text and a hypothesis, maybe a generated sentence. And the goal is to predict, whether the premise entails the hypothesis. That is the supporting text implies the generated sentence. There exists some off the shelf models that perform this kind of prediction, one of which is called true.

And they work relatively well. However, they tend to be pretty conservative. At the same time, new grounded versions of question answering have been proposed, which is the task of answering questions while also citing the sources of the answer being provided. And more work has focused on citation and attribution of LLM-generated content.

In this way, we can see that the research community thinks of hallucination as a serious problem and, as such, is devoting significant resources to study the phenomenon and trying to figure out ways to mitigate or avoid it. This concludes our discussion on hallucination. In our final lesson, we'll discuss applications of large language models.
==============================================================================================================================
LLM Applications:

Hello, welcome to the final lesson in the module covering large language models. So far, we've talked about LLM architectures, prompting, training, and decoding. We've also talked about some of the things to watch out for like memorization, prompt injection, and hallucination. In this lesson, I'm going to focus on a few applications of LLMs.

The first system that we'll cover is Retrieval Augmented Generation, otherwise known as RAG. This system is conceptually very simple. When people talk about RAG systems, typically, they're talking about a system where, first, a system user provides an input, for example, a question. Second, the system transforms that question into a query which will be used to search a database, for example, a corpus of documents.

The hope is that the search will return documents that contain the answer to the question or are otherwise relevant. Finally, the returned documents will be provided to the LLM as input in addition to the question. And the expectation is that the model will generate a correct answer. As I mentioned in the previous lesson, there is some work that shows that RAG systems, as opposed to systems that do not leverage an external corpus of documents, tend to hallucinate less. This may be intuitive.

If we give the LLM a question and then some text that contains the answer, it should be easier to answer the question by leveraging the text than answering it based solely on the documents it has seen during pre-training. RAG systems are powerful. For example, they can be used in multi-document question-answering. RAG systems are also more and more prevalent. They're used for a variety of tasks, such as dialogue, question-answering, fact-checking and others.

These systems are also elegant because they provide a non-parametric mechanism for improvement. By non-parametric, what I mean is that we don't have to touch the model at all to improve the system. All we have to do is add more documents. Let's think about this in a bit more detail.

Let's say that we have an LLM that can correctly answer a question provided that the context containing the answer is given to the model as input. Then you could take the model and answer any question you like, provided that you have a relevant corpus of documents to search and a sufficiently performant search engine. To make this more concrete, let's say that you care about answering customer questions about some software system that they are using.

In theory, in the RAG setup, all you need to do is provide the software documentation or manual as the corpus. And your LLM can now answer any question that can be answered with that manual. In practice, getting these systems to work is not trivial, because there are a few moving parts. But we've already seen a lot of RAG systems deployed in practice. And the performance of these systems seems to be improving, and the systems are ubiquitous across industry and academia. Moreover, we've seen them built on top of off-the-shelf LLMs as well as LLMs trained specifically for RAG.

Next, I'll briefly touch on LLMs for code, typically referred to as code models. As their name implies, code models are LLMs trained on code, comments, and documentation. These models have demonstrated amazing capabilities in terms of code completion and documentation completion. That is, if you provide the model with a description of the function you'd like to write, in many cases, it can just output the function for you. Some examples of these models you may have heard of include Co-pilot, Codex, and Code Llama.

Arguably, code completion might be easier than general text completion. And as such, it's easier to train performant code models. One potential reason for this is that generating code is narrower in scope than generating arbitrary text. Code is more structured. It's perhaps more repetitive and less ambiguous than natural language. Regardless of the reasons, many developers have claimed to have benefit significantly from the incorporation of code models into their workflows.

Anecdotally, these models largely eliminate the need to write any boilerplate code and commonly written functions or variables or variations of such functions. Moreover, they also shine when programming in a language that you don't know well. Instead of using a search engine to continually look up language syntax and functions, just ask the model to write the code for you.

On the flip side, more complicated tasks are still difficult or unattainable for code models. While generating code from scratch might be achievable, there is new work that shows that our best models can only automatically patch real bugs less than 15% of the time.

I want to briefly mention multi-modal models as well. These models are trained on multiple data modalities, for example text, images, and audio. These models can produce images or even video from textual descriptions and perform similar types of tasks. One interesting offshoot of this work that I'd like to make you aware of is some interesting advanced decoding techniques.

In particular, let's discuss diffustion models. If you recall, the way LLMs generate text is one word at a time. By contrast, diffusion models generate usually images all at once rather than one pixel at a time. The way they do this is by starting with an image that is simply noise-- it's not an image of anything at first-- and iteratively refining all the pixels in the image simultaneously until a coherent image emerges.

There have been some attempts at doing such a simultaneous or technically, what we would call, a joint decoding for text. But these approaches haven't achieved state-of-the-art results and are not yet popular. In fact, jointly decoding text is quite difficult, whereas, an image is a fixed size and has a fixed number of pixels. And we might know that size before beginning to generate. We typically don't know how many words we're going to generate in a sentence that we want to generate, in general. Moreover, whereas the pixels in an image take on continuous color values, words are discrete and, thus cannot be refined in a continuous manner.

Finally, I'll mention language agents. Language agents are models that are intended for sequential decision-making scenarios, for example playing chess, operating software autonomously, or browsing the web in search of an item expressed in natural language. Language agents are an extension of the classic work on machine learning agents. Yet, in this newer rendition, the systems being built also utilize LLMs.

In more detail, these models operate in what's known as an environment and iteratively take actions in pursuit of accomplishing a specific goal. For example, a language agent tasked with buying an ambiguously described product might take an action corresponding to a search. Every time the model takes an action, like searching, the environment responds, for example, with the search results.

The agent observes the results and takes another action, for example, visiting a page corresponding to a promising item. The model continues taking actions until it thinks that it has achieved its goal, at which point, it terminates. One reason that the interest in language agents is rapidly increasing is because their out-of-the-box capabilities for communication via natural language and their instruction-following capabilities. This makes it relatively easy to tell the model what it's supposed to do and what actions are available to it.

One of the canonical works in this space is known as ReAct which proposes a framework for leveraging LLMs as language agents. A key ingredient of this work is to prompt the model to emit what they call thoughts, which are summaries of the goal, what steps the model has already accomplished, and what steps the model thinks it needs to take next.

Perhaps because of the significant interest in language agents, there has been significant study of teaching LLMs how to leverage tools. Tools is used here very broadly, but boils down to using APIs and other programs to perform computation. For example, instead of doing some arithmetic by decoding, an LLM could generate some text expressing the intention to use a calculator, formulate an API call to perform the arithmetic, and then consume the result. The ability to use tools promises to greatly expand the capability of LLMs.

Finally, there is a growing body of work developing methods of training LLMs to perform various types of reasoning. LLMs that can reason successfully could be employed as high level planners in these agent systems for accomplishing highly complex, long-horizon tasks. Like humans, agents that can reason could be successful in new environments when trying to accomplish unfamiliar tasks.

This concludes the module on Large Language Models. I hope these lessons have provided an understandable while somewhat in depth technical discussion of LLMs, and I hope you learned something new. Thanks for joining.
==============================================================================================================================
