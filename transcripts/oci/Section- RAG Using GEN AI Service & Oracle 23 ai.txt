Section: RAG Using GEN AI Service & Oracle 23 ai vector search
==============================================================================================================================
OCI GEN AI Integrations
We already have learned that OCI Generative AI offers us a variety of features to generate, summarize, and embed data. Now, let us see how does it integrate with other useful frameworks and OCI services. We can leverage all these integrations to build useful applications. Open-source frameworks offer a multitude of components to build LLM-based applications, including an integration with OCI Generative AI service. Let us begin with the integration with LangChain.

LangChain is a framework for developing applications powered by language models. It enables applications that are context-aware and rely on language model to answer based on provided context. It offers a multitude of components that help us build LLM-powered applications with minimal effort. A few components that help us build applications are large language models, prompts, memory, chains, vector stores, document loaders, text splitters, and many others. These components are easily exchangeable as well. For example, we can switch between, say, one LLM with another LLM with minimal code changes.

The core element of any language model application is the model, of course. There are two main types of models that LangChain integrates with LLMs and chat models. These are defined by their input and output types. LLM in LangChain refers to pure text completion model. They take a string prompt as input and output a string completion. Chat models are often backed by LLMs but tuned specifically for having conversations. They take a list of chat messages as input, and they return an AI message as output.

Now, let us discuss prompts. In LangChain, prompts can be created using two types of LangChain prompt classes. First is prompt template. It is created from a formatted Python string that is a combination of fixed text and any number of placeholders that are filled in when code executes. We usually use this with generation models, but we can also use it with chat models. Second is chat prompt template. It is composed of a list of chat messages, each having a role and content. We use this with chat models.

LangChain provides frameworks for creating chains of components, including LLMs and other types of components. We can compose chains in two ways. We can use language chain expression language or LCEL. It is a declarative and preferred way to create chains, or we can create chains using LangChain Python classes like LLM chain.

Now, we shall see how LangChain model prompt, and chain is used together to get response from LLM, given a user query as input. The user query can be used to invoke an LLM directly to get a response. But most of the time, in addition to the user query, we also provide more context to the LLM, like instructions or information captured at runtime. That is, when code executes. To gather this additional context, we use prompts. Prompts combine the fixed text-like instructions and variables captured at runtime to create a prompt value. And then LLM accepts prompt value as input and generate a response.

So how do we call prompt and LLM in sequence? Using chains, we can string together operations like accepting an input, passing it onto a prompt, and to get a prompt value, and passing on the prompt value to LLM to get a response.

We will use memory to store conversation with the chatbot at a point in time. Chain retrieves the conversation that is a series of chat messages from the memory using key, and passes it to the LLM along with the question. And once the chain receives a new answer to the latest query, it writes back the query and answer to the memory.

LangChain offers a variety of memory types. It all depends on what is returned from the memory. For example, we may have a chain that returns a summary of the contents of the memory compared to the actual contents of the memory. Even extracted entities like names can also be returned. Oracle 23 ai can be used as a vector store, and LangChain offers Python classes to store and search embeddings in Oracle 23 ai vector store.

OCI generative AI is used by Oracle 23 ai in a few ways. First, to generate embeddings outside the database, you can access OCI Generative AI service using DB UTILs and REST APIs. Second, Oracle 23 ai SELECT AI can use OCI Generative AI service to generate SQL to query data stored in the database using natural language. And OCI Generative AI can be used via LangChain classes. As usual, applications that use both OCI Generative AI and Oracle 23 ai vector store can be created using Python SDK as well. Thanks for watching.

==============================================================================================================================
RAG
Let us understand what is Retrieval Augmented Generation. Traditional language models generate responses based solely on their training data, which can become outdated. RAG addresses this by retrieving up-to-date information from external sources and providing this additional and specific information to LLM, along with the user query, thus enhancing the context provided to the LLM for generating more relevant response.

A few benefits of this approach are standard LLMs can sometimes carry forward biases or errors present in their training data. RAG can mitigate this by pulling in a variety of perspectives and sources, leading to more balanced and accurate responses. RAG can also overcome model limitations such as token limits since we are only feeding top K search results to the LLMs instead of the whole document. RAG allows models to handle a broader range of queries without the need for exponentially larger training data sets.

Let us see how a basic Retrieval Augmented Generation pipeline looks like. Let us see step-by-step explanation of the process. Ingestion, this is the first phase where documents are ingested into the system.

Here we begin with loading the documents. Documents are the original text corpus. The documents are broken down into smaller, more manageable pieces, often referred to as chunks.

This is typically done to focus on relevant sections of the text. Each chunk is then transformed into a mathematical representation called as embeddings. These embeddings capture the semantic information of the text and allow for comparisons to be made in a numerical space. The embeddings are indexed in a database that facilitates quick retrieval. The index is a data structure that allows the system to find and retrieve embeddings efficiently when a query is made.

Retrieval, in this phase, the system uses the index data to find relevant information. Retrieval begins when a user inputs a question that needs to be answered. The system uses the query to search through the stored indexed embeddings to find the most relevant chunks.

From the retrieved documents, the system selects the top K, that is a preset number of the most relevant results. These are the chunks that are most likely to contain information relevant to the query. Generation, this is the final phase where the system generates a response based on the information retrieved. The selected chunks from the retrieval phase are fed into the generative model.

The generative model, often a neural network like a transformer, uses the context provided by the top K chunks to generate a coherent and contextually relevant response to the query. The RAG architecture is very effective in scenarios where generative models need to be supplemented with specific information that may not be present in the training data. Thanks for watching.

==============================================================================================================================
Process Documents
In the previous lesson, we discussed that RAG pipeline consists of ingestion, retrieval, and generation. Now let us discuss each of these in detail. We will begin with ingestion. The first step in ingestion is to load documents. The documents can come from a variety of sources and have multiple formats. The documents can be PDFs, comma-separated values, HTML, JSON, and many other types.

Most of the LLM frameworks, including LangChain, offer classes to load different types of documents. The loader classes also support loading just a single document or all the documents in a given directory. Once the documents are loaded, the next step is to split the documents into smaller pieces, also referred to as chunks.

There are a few things to consider while splitting the documents. Let us understand each of these. First, consideration is the size of the chunk. That is how big or small the chunk should be. Most of the LLMs have a maximum input size constraints, so the maximum size of the chunk is limited by the context window of the LLM.

If we split the document into smaller chunks, it helps to fit the documents in the context window of the LLM. But note that if we make chunks too small, they may not be semantically useful. And if we make them too big, they may not be semantically specific.

Next consideration is to maintain the continuity of the context from one chunk to another. For this, we include a part of the preceding chunk into the next chunk. This is referred to as chunk overlap. So if we consider just one chunk, it also has some reference to the previous chunk. This helps improve the continuity of the context.

The next consideration is how do we split the document, given the chunk size to keep the chunks semantically meaningful. If we consider a block of text, usually it already has a semantic structure. For example, a paragraph may explain a concept, and then each sentence in a paragraph may be semantically meaningful. Text splitter also uses similar concept. They try to split the block of text using separators like paragraph separator or sentence separator or even a word separator.

So to get a chunk of a given size, text splitter first tries to retain paragraphs-- if not, at least sentences together-- and so on. The idea is to get the chunks of a given size, but also to get semantically richer chunks. Let us go through sample code to load and split a PDF document. Here we use PDF reader class to read a PDF document.

Once we have a reader object, we can use it to extract text from all the pages of the PDF and get a text variable. Next, we create a text splitter object. We pass in the chunk size and chunk overlap parameters to the text splitter object. Next, we call upon the split text method of the text splitter object to get chunks from a text variable we created earlier. Thanks for watching.

==============================================================================================================================
Embed & Store Documents


In the previous lesson, we discussed how documents are loaded and split into chunks. Now let us see how chunks are embedded and stored for retrieval. But before that, let us understand what are embeddings.

If we take an example of three group of words, say animals, fruits and places and are given a word tiger. We as human beings will place it in the animals group because we know that semantically, tiger is similar to the words in the animal group. But for the machines to understand the similarity of words or sentences or even documents, concept of embeddings was born.

The embeddings of similar words or sentences or documents are close by in the multidimensional space. This is achieved through a process of training the embedding building models. One string embeddings reflects semantic similarity of words or sentences or documents.

What we see in the picture is a two dimensional representation of the embeddings of a few words. If we measure the similarity of a new word, say tiger with the other words, it would be closer to the words in the animals group than the words in the fruits or places group.

Here we see four words and their corresponding numerical representations. Embeddings of the semantically similar words are close by. We can also have embeddings for sentences, paragraphs, or even documents, which can be used to compare semantic similarity.

Now that we understand what embeddings are, let us see how we can generate embeddings from the chunks. Embeddings are created using train embedding model. Oracle 23ai supports using embedding model inside or outside the database. For generating embeddings outside the database, third party embedding models can be used.

If you want to keep the data within the database itself, then we can import ONNX that is ONNX format embedding models in a database 23ai and embed the chunks within the database itself. Now that we know how embeddings are created, let us see how we can store them.

Oracle 23ai introduced a new data type vector to store embeddings in a database column. So we can create a column of type vector data type along with the columns of other data types, while creating a database table and store embeddings into this column. We can use usual database inserts or update statements to create records that contain vector data type.

So far, we have split documents and created chunks. Now let me walk you through the code to embed chunks and store these in a Oracle 23ai vector store. For this, we need to create a database connection in the first place. We use username, password, and data source name as input parameters and pass it to Oracle DB connect method.

Once we have database connection object, we are ready to proceed further. We have created chunks earlier by splitting documents. In order to store these in a database, we need to convert these to document objects that have a page content and metadata.

For this, we iterate through chunks, extract page number, and actual text content of the chunk, and then create a dictionary using the ID, link, and text as keys. We pass in this dictionary to the chunks to Docs wrapper function. This function creates a metadata and page content and returns a document which wraps metadata and page content.

Finally, we have a list of documents which correspond to chunks we had. Now we can embed and store these. For this, we create an embedding model using OCI Generative AI service. We pass in the model name, service endpoint, compartment name, and authtype as parameters.

Finally, we create a vector store using Oracle VS class and from documents method. We pass in a list of documents that we created, embedding model, database connection, name of the table, and distance strategy, which is a way to compare embeddings.

Now we are ready to use vector store for searching documents that match the user query. Thanks for watching.
==============================================================================================================================
Retrieval & Generation

So far, we have loaded documents, split them into chunks, embedded chunks, and stored them in the database. Now let us discuss retrieval. When a user enters a query that we want LLM to answer, we first encode the query using the same embedding model we used to encode the chunks.

Next, we fire a search on our database where embedded chunks are stored with the encoded query. The idea here is to get back chunks that are similar to our query so that these will provide additional context to our query. The search may return multiple chunks. And vector search will return a few of these to keep the context short and relevant.

The question here could be that-- how does vector search find similar chunks to the query. For this, vector search typically uses two similarity measures-- dot and cosine distance. Using these similarity measures, we can compare query embeddings and the stored chunk embeddings.

Dot product measures the magnitude of the projection of one vector onto the other. Dot product considers magnitude and angle between vectors to calculate similarity. Cosine product considers only the angle between the vectors and not the magnitude to calculate similarity.

In the context of NLP, the more magnitude may mean semantically richer content, and less angle means more similarity. We discuss that we need to compare user query embedding with the embedding of each chunk. This is fine when we have smaller number of junk embeddings. But as the number of chunks grow, we need a performant way to search for the similar embeddings.

This is where index come in handy. Indexes are like table of content. Using indexes, embeddings can be found easily. Indexes are specialized data structures designed for similarity searches. Various techniques like clustering, partitioning, and neighbor graph are used to group embeddings. This helps reduce the search space.

A hierarchical navigable small-world graph, that is HNSW, is a form of in-memory neighbor graph vector index. It is a very efficient index for vector approximate similarity search. Inverted File Flat, or IVF, is a form of neighbor partition vector index. It is a partition-based index that achieves search efficiency by narrowing the search area through the use of neighbor partitions or clusters.

Once we have retrieved the context in the form of relevant chunks, we can send it to the LLM along with the query. And finally, LLM considers the context and user query to get us a relevant and specific response. Now let me walk you through the retrieval code.

First we import the necessary classes. That is RetrievalQA, ChatOCIGenAI, and OracleVS. Next, we create a vector store using OracleVS class, and we pass in the embedding model, database connection, table name, and distance strategy. Next, we create a retriever, and we pass in the search type as similarity and search kwargs as k colon 3, which will return top three results.

Next, we create the LLM using ChatOCIGenAI. We pass in the model id, service endpoint, compartment id, auth as required. Next, we create chain using RetrievalQA class, and we pass in LLM retriever. And we set return source document as true, which will help us return the source documents along with the response. And finally, we invoke a chain with our question, and we get a response. Thanks for watching.
==============================================================================================================================
Demo: Langchain Basics


In this demo, we will explore a few LangChain components, like models, prompts, chains, and memory. So let us begin with models. We import Chad OCI Gen AI class from LangChain community package, which represents OCI Generative AI service.

This class allows us to invoke generative AI service using LangChain. We create LLM object using chat OCI Gen AI class. We pass in the model name. We pass in the service endpoint, which is a network access point for generative AI service in a specific region.

We pass in the compartment ID. We also pass in the max tokens as 200, because we want to limit the output of the LLM. So let us execute this cell.

What we do next is, we call upon the invoke method of the LLM. And we pass in the question that we want to ask. We also pass in a temperature parameter, which decides the creativity of the output. And we print the response from the LLM.

So let's execute and see what happens. We got back the response from the LLM. And it is a fact about the space.

Next, what we will do is we will learn to use a prompt. For that, we create a template. Template is a formatted Python string, where we can combine a fixed text, as you can see here. And the variable text, which you can capture at the runtime, will create an instance of the prompt template. And we declared two input variables-- user input and city. And we pass in the formatted template as well.

We can get a prompt value by calling upon invoke method on the prompt and giving the necessary inputs. And what we do here is we print the prompt value. So let us execute and see the results.

So as you can see here, the printed prompt value is the combination of the fixed text that we had given. So this was the fixed text that we had given. And these were the variables that we had passed in. And this is what is combined and we get a prompt value.

What we do next is we chain the prompt and LLM using a pipe operator. So what happens is, the input is given to the prompt, and the output of the prompt is given to the LLM. Then finally, the LLM will respond back. And we call upon the invoke method on the chain object. We pass in the necessary inputs. And we print the response.

So our question was, tell us in an exciting tone about New York. And this is the response we are getting from the LLM. Next, what we do is we use the chat prompt template, which allows us to use a list of messages. Similarly, we call upon the invoke method of the chat prompt. So as we can see here, the prompt value is a list of messages in this particular case.

What we do here is, once again, we chain the prompt and the LLM. And we invoke a chain by giving a input. In this particular case, the input is, what's the New York culture like? And we print the response. So once again, we can see the response here.

Next, we will learn to use memory. For that, we import conversation buffer memory and conversation chain. And we create an instance of the conversation buffer memory. And we create an instance of the conversation chain by passing in the LLM and the memory to it.

What we do here is we invoke a conversation with a question. And we print the contents of the memory. So this is the result of the first print, where we are showing the results of the invocation of the conversation. So here, we can see whatever input we are given. There is no history because there was no prior question. And this is the response.

And these are the memory contents. So this is the question that we had asked. And this is what the AI, that is the LLM, and responded back with it.

What do we do next is, we ask another question-- can you tell what is my name? So we expect the conversation to remember that the name is Hemant. So once again, this is the result of the invocation of the chain.

And in the history, we can see that the first question was, Hello, my name is Hemant. And this is the response that LLM had given for the first question. And these are the memory contents, which is a question and the response to our first question. This is our second question. And this is the response that we got back from the LLM.

And the response is, Certainly, Hemant. Your name is Hemant. So this means that the LLM remembered the first question and responded based on this. Thanks for watching.
==============================================================================================================================
Conversational RAG


So far, we discussed what RAG is and how RAG pipeline is implemented. We now understand how RAG helps us improve the context of our query by retrieving relevant documents and sending these to the LLM for getting more relevant and specific response to our query. RAG is used often to create chatbots.

Chat is a series of question and answers. A user will ask a question. LLM will provide an answer. And then user will follow up with a next question and so on.

RAG is used to answer the questions by using the relevant information from the provided text corpus. In case of a chat, even the prior questions and answers may act as an additional context to the next question. For example, if we ask, tell me about Las Vegas, and the next question is, tell me about its typical temperature throughout the year, the second question is referring to Las Vegas.

In order to maintain a list of question asked and answers given, a concept of memory is used. The contents of the memory are updated every time a new question is asked and an answer is generated. The memory contents are passed on as additional context to the LLM. LLM answers the next question considering the retrieved documents and conversation history, LangChain provides a variety of memory and chain classes that help implementing conversational chatbots. Thanks for watching.
==============================================================================================================================
Demo:RAG with Oracle Database 23 ai


In this demonstration, we will see how to use RAG that is Retrieval Augmented Generation with Oracle 23ai. For that, we will begin with the creation of autonomous database. We are logged in into the console of autonomous database. And we will click on Create Autonomous Database.

Here, we will provide the display name and the database name. And both can be same as well. And we'll also make sure that we are selecting the appropriate compartment. We'll choose the data warehouse, type of workload. And we will choose the deployment type as serverless.

In the Configure the Database section, we will leave all the default values as is. In the Create Administrator Credentials section, we will provide the password for the admin user. We will choose the secure access from allowed IPs, and we will choose our own IP.

And we will leave the required mutual TLS, that is mTLS authentication unchecked. If you want to know more about mutual authentication, you can learn using this link. And rest of the things, we will leave as default and click on Create Autonomous Database.

Once the database is created, we can check the access control list here, and we can see that our IP has been whitelisted here. If we choose the database connection, we can copy the connection string for our database. And we will require this connection string when we connect to this database using oracledb Python library.

Now let us see how to implement retrieval augmented generation with Oracle 23ai. So for that, we begin with the import of necessary classes. Next, we declare the username, and the password, and the data source name for the Oracle 23 database that we have created.

Next, we connect to the database using the oracledb Python library and we use the connect method, passing the username, password, and the data source name to it. Once we get a successful connection, we create a PdfReader object.

And we use that PdfReader object to read the document that we have passed in to the reader. So we scan through all the pages and extract the text from that page and create a text object out of it.

Next, what we do is we create a text character splitter object. So we pass in the separator chunk_size and the chunk_overlap as parameters to it. So separator decides how the text is split.

So if we have given the separator as full stop, that means whenever the character text splitter encounters the full stop, it is going to split the text over there. And if it doesn't encounter a full stop, then it will basically go till the chunk_size that we have specified, and then it will split the text.

The chunk_overlap basically ensures that the context is maintained from one chunk to the another. So once we have split the text into the chunks, what we do next is actually we need to convert the chunks into the documents which are ingestible into the Oracle Vector Store.

So we have to create a document object out of the chunks that we have. So for that, what we do is we have declared a function here, and we pass a dictionary object to it. So from the dictionary object that we pass, the metadata is going to be extracted, and the document object is going to be created out of it using the metadata that is created and the text.

Now, what do we do next is actually, we enumerate through all the chunks, extract the page number and text from it, and then basically create a dictionary object from this page number and the text object that we have extracted. And we basically pass it to the chunks_to_docs_wrapper, which is going to return a document to us.

So next, what we do is we declare the COMPARTMENT_OCID. We create a embedding model out of the OCIGenerativeAIEmbeddings object. We pass in the model_id. We pass in the service_endpoint and the compartment_id.

So finally we use the Oracle Vector Store objects and use the from_documents method. We pass in the documents that we had created. We pass in the embed_model that we had created. And this is a connection objects that we had created earlier. And we specify a table_name.

Now what's going to happen with the table_name is, the documents that we have provided, they will be embedded. And then those documents will be stored into the table that we are specifying here. And the distance_strategy is specified for calculating the distance between the two embedded document.

At the end of the execution of this code, what we are going to get is, we will be getting the documents embedded and stored into the Oracle Vector Store. So let's run the code and see what happens.

So as we can see here, our PDF has been transformed into the text format, and it has been stored into the Oracle Vector Store as well. We have connected to the database using Oracle SQL Developer. We can see our table here.

So there are four columns in it. So this is a primary key column. This is a Text column. This is a Metadata column. And this is Embedding column. And this is how the data looks like. So essentially, we have a primary key here. We have the text here. And we have the metadata here. And we have the embedding here.

Now that we have created the Oracle 23 Vector Store for us, what we will do is we will connect to the vector store and run some queries. So for that, we'll begin with the import of the classes. We will also declare the username, password, and the data source name.

We will connect to the database or using oracledb Python library, and we will use the connect method. We will create the ChatOCIGenAI object using the model_id, service_endpoint, compartment_id, and a couple of other model_kwargs, like "temperature" and "max_tokens."

And we'll also create a embed_model using OCIGenAIEmbeddings. We'll pass in the model_id, service_endpoint, and our compartment_id here as well.

Next, we'll create a template using which we can create the context that is our retrieved documents and the user questions and we will create a prompt using the template. So we will create the vector store object using the OracleVS Python class. And we will pass in the embedding function, the connection object, and the table name.

So we already have populated this table when we embedded and stored the documents into the Oracle Vector Store. And we'll also pass in the distance_strategy here, which will be used to calculate the similarity between two vectors.

Next, we'll create a retriever out of this vector store that we created. And we will use the search_type of "similarity." And we'll also use the search_kwargs, which specifies that we'll be using the top three documents.

Next, we create a chain. And if you notice, we are passing this retriever as a context because retriever essentially is going to be containing all the retrieved documents here. And we are passing in the user question as it is to the prompt. And for that, we are using this RunnablePassthrough.

And finally, we create a user_question, and we invoke a chain with that particular user_question. And we print the user_question and LLM response. So let's go ahead and run this code. So as you can see, this was our question here. "Tell us about Module 4 of AI Foundation Certification Course."

And the response is, "According to the provided context, Module 4 of the AI Foundation Certification Course is about Generative AI and LLMs," which is the right response.

We have seen how to take a PDF document, how to split it, how to store it in the Oracle Vector Store. And we have seen how to retrieve the documents from the Oracle Vector Store, which are matching the query. And we have also seen how a question can be answered based on the retrieved document.
==============================================================================================================================