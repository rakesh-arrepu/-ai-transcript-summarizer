# ============================================================================
# Transcript â†’ Exam Notes Pipeline - Configuration File
# ============================================================================
#
# This is an example configuration file.
# Copy this to the project root as '.env' and add your actual API keys.
#
# SECURITY WARNING: Never commit the .env file to version control!
# The .env file should be in .gitignore (it already is by default).
#
# ============================================================================

# ============================================================================
# API KEYS (REQUIRED)
# ============================================================================
# Get your keys from:
# - Anthropic: https://console.anthropic.com/
# - OpenAI: https://platform.openai.com/api-keys
# - Google AI Studio: https://aistudio.google.com/app/apikey

# Anthropic Claude API Key
# Format: sk-ant-xxxxxxxxxxxxx
CLAUDE_API_KEY=your_claude_api_key_here

# OpenAI API Key
# Format: sk-xxxxxxxxxxxxx
OPENAI_API_KEY=your_openai_api_key_here

# Google Gemini API Key
# Format: AIzaSy... (from Google AI Studio)
GEMINI_API_KEY=your_gemini_api_key_here


# ============================================================================
# API ENDPOINTS (OPTIONAL)
# ============================================================================
# Use default values unless you're using a proxy or alternative provider.
# Defaults point to official API endpoints.

# Anthropic API Endpoint
# Default: https://api.anthropic.com/v1
CLAUDE_API_BASE=https://api.anthropic.com/v1

# OpenAI API Endpoint
# Default: https://api.openai.com/v1
OPENAI_API_BASE=https://api.openai.com/v1

# Google Gemini API Endpoint (OpenAI-compatible)
# Default: https://generativelanguage.googleapis.com/v1beta/openai/
GEMINI_API_BASE=https://generativelanguage.googleapis.com/v1beta/openai/


# ============================================================================
# MODEL SELECTION
# ============================================================================
# These should match available models from your API provider.

# Claude Model to use for chunking and summarization
# Recommended: claude-3-5-sonnet-20241022 (best quality)
# Alternative: claude-opus-4-1 (slower but more capable)
# Alternative: claude-3-5-haiku-20241022 (faster, cheaper)
MODEL_CLAUDE=claude-3-5-sonnet-20241022

# GPT Model to use for consolidation and exam materials
# Recommended: gpt-4o (best quality, moderate cost)
# Alternative: gpt-4-turbo (more expensive)
# Alternative: gpt-3.5-turbo (cheaper, lower quality)
MODEL_GPT=gpt-4o

# Gemini Model to use (for both summarization and consolidation)
# Recommended: gemini-2.5-pro (latest, best quality)
# Alternative: gemini-2.0-pro (previous version)
# Alternative: gemini-1.5-pro (older, still capable)
MODEL_GEMINI=gemini-2.5-pro


# ============================================================================
# PIPELINE MODEL SELECTION (FLEXIBLE)
# ============================================================================
# Choose which model to use for each pipeline step.
# Allows mixing and matching for cost optimization.

# Model to use for chunk summarization
# Options: claude (recommended), gemini
# Default: claude
SUMMARIZER_MODEL=claude

# Model to use for consolidation and exam materials generation
# Options: gpt (recommended), gemini
# Default: gpt
CONSOLIDATOR_MODEL=gpt

# Cost comparison per lecture:
# Claude + GPT (default): ~$2.15
# Claude + Gemini: ~$0.50 (much cheaper!)
# Gemini + Gemini: ~$0.10 (cheapest, requires experimentation)
#
# Recommendation:
# - For best quality: claude + gpt (default)
# - For cost savings: claude + gemini
# - For maximum savings: gemini + gemini (test first!)


# ============================================================================
# PIPELINE CONFIGURATION
# ============================================================================

# Directory containing input transcript files (*.txt)
# Default: transcripts
TRANSCRIPT_DIR=transcripts

# Directory where all outputs will be saved
# Default: output
OUTPUT_DIR=output

# Directory where logs will be written
# Default: logs
LOGS_DIR=logs


# ============================================================================
# CHUNKING PARAMETERS
# ============================================================================
# These control how transcripts are split into chunks for processing.

# Target size for each chunk in estimated tokens
# Smaller = more chunks, more API calls, better context per chunk
# Larger = fewer chunks, fewer API calls, less context per chunk
# Recommended: 1500 (balanced)
# Range: 500-3000
CHUNK_SIZE=1500

# Token overlap between consecutive chunks
# Helps maintain context across chunk boundaries
# Typical: 10-20% of CHUNK_SIZE
# Recommended: 200
CHUNK_OVERLAP=200


# ============================================================================
# API BEHAVIOR CONFIGURATION
# ============================================================================

# Maximum time to wait for API response (in milliseconds)
# Default: 60000 (60 seconds)
# Increase for slow connections or large requests
# Decrease for faster failure detection
API_TIMEOUT=60000

# Number of retry attempts for failed API calls
# Default: 3
# Higher = more resilient to transient errors but slower
# Lower = fail faster but less resilient
MAX_RETRIES=3

# Initial backoff time for retries (in milliseconds)
# Uses exponential backoff: backoff * 2^attempt_number
# Default: 1000 (1 second)
# First retry waits 1s, second 2s, third 4s, etc.
RETRY_BACKOFF=1000


# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
# SLF4J Simple Logger Configuration
# Levels: DEBUG < INFO < WARN < ERROR

# Log level for console output
# Default: INFO
# DEBUG = Very detailed (development)
# INFO = Important events
# WARN = Warnings and errors
# ERROR = Only errors
org.slf4j.simpleLogger.defaultLogLevel=INFO

# Show thread name in logs
org.slf4j.simpleLogger.showThreadName=true

# Show date/time in logs
org.slf4j.simpleLogger.showDateTime=true

# Date/time format for logs
org.slf4j.simpleLogger.dateTimeFormat=yyyy-MM-dd HH:mm:ss


# ============================================================================
# ADVANCED OPTIONS
# ============================================================================

# Set to true to use API-based chunking (slower, better quality)
# Set to false to use local semantic chunking (faster, good quality)
# Default: false (local chunking)
USE_API_CHUNKING=false

# Enable detailed logging for API calls
# Useful for debugging API issues
DEBUG_API_CALLS=false

# Maximum tokens to send to API (safety limit)
# Default: 100000
# Longer requests are automatically truncated
MAX_REQUEST_TOKENS=100000

# Temperature for GPT responses (controls randomness)
# Range: 0.0 (deterministic) to 2.0 (random)
# Default: 0.7 (balanced)
# Lower = more consistent, Higher = more creative
GPT_TEMPERATURE=0.7


# ============================================================================
# OPTIONAL: CUSTOM PROMPTS
# ============================================================================
# Uncomment to use custom prompts instead of built-in ones.
# Useful for specific domains or languages.

# CUSTOM_CHUNK_SUMMARIZER_PROMPT=file://path/to/custom_prompt.txt
# CUSTOM_CONSOLIDATOR_PROMPT=file://path/to/consolidator_prompt.txt


# ============================================================================
# NOTES
# ============================================================================
#
# 1. API Keys:
#    - Never share or commit your .env file
#    - Use strong, unique keys for production
#    - Rotate keys regularly
#    - Monitor API usage for unusual activity
#
# 2. Cost Estimation:
#    - Claude Input: ~$3/1M tokens, Output: ~$15/1M tokens
#    - GPT-4o Input: ~$5/1M tokens, Output: ~$15/1M tokens
#    - Typical pipeline: $1-3 per lecture
#    - See README.md for detailed cost breakdown
#
# 3. Rate Limiting:
#    - Default 1 second between API calls
#    - Increase RETRY_BACKOFF if hitting rate limits
#    - Most free tier: 3 req/min, 200K tokens/min
#
# 4. Performance:
#    - Chunking: <1 minute
#    - Summarization: 30-60 sec per chunk
#    - Consolidation: 2-5 minutes
#    - Total: 15-30 min per lecture (with network latency)
#
# 5. Troubleshooting:
#    - Invalid API key: Check key format and provider
#    - Rate limit: Increase RETRY_BACKOFF or reduce chunk size
#    - Timeout: Increase API_TIMEOUT or check connection
#    - Low quality: Use larger CHUNK_SIZE or better models
#
# ============================================================================
