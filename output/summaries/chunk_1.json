{
  "chunk_id" : "1",
  "title" : "Fundamentals of Large Language Models - Introduction and Architecture",
  "summary" : "This chunk introduces large language models (LLMs) as probabilistic models of text that compute probability distributions over vocabulary words. It explains that LLMs are not fundamentally different from regular language models, with 'large' referring to parameter count without agreed thresholds. The module covers three main technical areas: architecture (encoders vs decoders), methods to affect distributions (prompting and training), and decoding for text generation.",
  "key_points" : [ "LLMs are probabilistic models that assign probabilities to words in their vocabulary", "The term 'large' in LLM refers to parameter count but has no agreed-upon threshold", "Two main architectures exist: encoders (for embeddings) and decoders (for text generation)", "Both architectures are built on transformer models from the 2017 'Attention Is All You Need' paper", "Model size is measured by number of trainable parameters, which can vary by orders of magnitude" ],
  "workflows" : [ {
    "name" : "LLM Text Processing",
    "steps" : [ "Input sequence of words to LLM", "Model computes probability distribution over entire vocabulary", "Each word in vocabulary receives a probability score", "Distribution can be used for various downstream tasks" ],
    "notes" : "Only words within the model's vocabulary receive probabilities"
  } ],
  "definitions" : [ {
    "term" : "Language Model",
    "definition" : "Probabilistic models of text that compute distributions over vocabulary words"
  }, {
    "term" : "Embedding",
    "definition" : "Process of converting text sequences into numeric vector representations that capture semantic meaning"
  }, {
    "term" : "Encoder",
    "definition" : "Architecture designed to encode text into embeddings or vector representations"
  }, {
    "term" : "Decoder",
    "definition" : "Architecture designed to generate or decode text sequences"
  } ],
  "examples" : [ "Children's book sentence: 'I wrote to the zoo to send me a pet. They sent me a--' with LLM predicting next word", "BERT model being referred to as an LLM despite potentially smaller size", "Transformer architecture from 'Attention Is All You Need' (2017) as foundation for modern LLMs" ],
  "exam_pointers" : [ "LLMs assign probabilities to ALL words in vocabulary, not just likely ones", "Encoder models produce embeddings, decoder models generate text", "All modern LLMs are built on transformer architecture from 2017", "Model size is measured by trainable parameters and varies by orders of magnitude" ],
  "confidence" : "high"
}