{
  "chunk_id" : "2",
  "title" : "Language Model Architectures and Prompting Introduction",
  "summary" : "This chunk covers the three main language model architectures: encoders (designed for text embedding like BERT), decoders (for text generation like GPT-4), and encoder-decoder models (for translation tasks). It introduces prompting as a method to control model output distributions and explains how decoder models generate text one token at a time through iterative processes.",
  "key_points" : [ "Decoder models are historically larger than encoders due to text generation requirements", "Encoders convert text sequences to vector representations for classification or semantic search", "Decoder models generate text one token at a time through iterative token prediction", "Encoder-decoder models combine both architectures for sequence-to-sequence tasks like translation", "Prompting is the primary method to control model output probability distributions" ],
  "workflows" : [ {
    "name" : "Semantic Search with Encoders",
    "steps" : [ "Encode/embed each document in corpus and store in index", "Encode input snippet when query arrives", "Compare similarity between encoded input and all corpus documents", "Return most similar documents" ],
    "notes" : "Used for document retrieval and vector search in databases"
  }, {
    "name" : "Text Generation with Decoders",
    "steps" : [ "Feed initial sequence of tokens to decoder model", "Model produces next token based on vocabulary probabilities", "Append generated token to input sequence", "Feed extended sequence back to model for next token", "Repeat until desired sequence length achieved" ],
    "notes" : "Computationally expensive due to model size and iterative process"
  }, {
    "name" : "Translation with Encoder-Decoder",
    "steps" : [ "Send source language tokens to encoder", "Encoder embeds all tokens and sentence", "Pass embeddings to decoder", "Decoder generates target tokens one at a time", "Each generated token feeds back for next token generation" ],
    "notes" : "Self-referential loops in decoder for sequential generation"
  } ],
  "definitions" : [ {
    "term" : "Encoder",
    "definition" : "Model designed to embed text by converting word sequences to vector representations for downstream tasks"
  }, {
    "term" : "Decoder",
    "definition" : "Model that takes token sequences and emits the next token based on computed vocabulary probabilities"
  }, {
    "term" : "Semantic Search",
    "definition" : "Vector-based search method using encoded text representations to find similar documents in a corpus"
  }, {
    "term" : "Prompting",
    "definition" : "Method to alter probability distributions of vocabulary words in language model outputs"
  } ],
  "examples" : [ "BERT as an encoder model for text embedding", "GPT-4, Cohere command model, and Llama as decoder models", "English-to-foreign language translation using encoder-decoder architecture" ],
  "exam_pointers" : [ "Decoder models generate only one token at a time through iterative processes", "Encoders are used for embedding, not text generation", "Encoder-decoder models are primarily used for sequence-to-sequence tasks", "Prompting is the primary method to control model output distributions" ],
  "confidence" : "high"
}