```markdown
# Comprehensive Study Material on Large Language Models

## 1. Master Notes

### A. Fundamentals of Large Language Models (LLMs)
- **Definition**: LLMs are probabilistic models that compute distributions over vocabulary words.
- **Key Concept**: 'Large' refers to the parameter count, with no standard threshold defined.
- **Architectures**:
  1. **Encoder**: Converts text into vector representations (e.g., BERT).
  2. **Decoder**: Generates text sequences one token at a time (e.g., GPT-4).
  3. **Encoder-Decoder**: Combines both for tasks like translation.

**Example**: BERT uses encoders for tasks like sentiment analysis, while GPT-4 uses decoders for generating creative writing.

### B. Language Model Architectures
- **Encoder**: Embeds text for classification and semantic search.
- **Decoder**: Emits the next token based on probability distributions.
- **Prompting**: Alters model outputs by changing input text structures.

### C. Prompting and In-Context Learning
- **Prompting**: Changes input text to influence output distributions.
- **Prompt Engineering**: Iterative refinement of model inputs for desired outputs.
- **In-context Learning**:
  1. **Zero-shot**: No examples provided.
  2. **K-shot**: Includes k examples to guide the model.

### D. Advanced Prompting Techniques
1. **Chain-of-Thought Prompting**: Breaks complex problems into smaller steps.
2. **Least-to-Most Prompting**: Solves simpler tasks first to build solutions.
3. **Concept-Based Prompting**: Uses first principles for scientific problem-solving.

### E. Training Methods and Prompt Injection
- **Prompt Injection**: Manipulating inputs to elicit unintended outputs.
- **Training Approaches**:
  - **Fine-tuning**: Adjusts all model parameters for a specific task.
  - **Parameter-efficient methods (e.g., LoRA)**: Modify only certain parameters.

### F. Soft Prompting and Decoding Strategies
- **Soft Prompting**: Adds trainable parameters to cues for specific tasks.
- **Decoding**: Generates text iteratively; methods include:
  1. **Greedy Decoding**: Chooses highest probability word at each step.
  2. **Nucleus Sampling**: Samples from a defined portion of the distribution.
  3. **Beam Search**: Generates multiple sequences and selects the best.

### G. Hallucination and Mitigation Strategies
- **Hallucination**: Generation of text not grounded in training data, often factually incorrect.
- **Mitigation**:
  - **Retrieval-Augmented Generation (RAG)**: Uses external documents for context.
  - **Code Models**: Excel at structured tasks like code completion.

### H. Limitations and Advanced Applications
- **Limitations**: Code models fix bugs only 15% of the time.
- **Multi-modal Models**: Handle text, images, and audio.
- **Language Agents**: Execute sequential actions in decision-making tasks.

## 2. Quick Revision
- LLMs compute word probabilities; 'large' refers to model size.
- Encoder (BERT) vs Decoder (GPT-4) architectures.
- Prompting alters output distributions; prompt engineering refines inputs.
- In-context learning methods: zero-shot and k-shot prompting.
- Advanced prompting techniques: chain-of-thought, least-to-most, concept-based.
- Prompt injection can exploit LLM vulnerabilities.
- Training methods: fine-tuning (all parameters) vs LoRA (specific parameters).
- Soft prompting adds trainable parameters; greedy decoding selects highest probability words.
- Hallucination is generated text without factual grounding; RAG reduces hallucination risks.
- Code models handle structured tasks; limitations include poor bug fixing.
- Multi-modal models integrate various data types; language agents support sequential decision-making.

## 3. Practice Questions

### Multiple Choice
1. What do LLMs compute?
   - A) Text distributions
   - B) Image distributions
   - C) Sound distributions
   - D) None of the above
   - **Answer**: A*

2. Which architecture is designed for text generation?
   - A) Encoder
   - B) Decoder
   - C) Both
   - D) None
   - **Answer**: B*

3. What is 'prompt engineering'?
   - A) Training the model
   - B) Refining model inputs
   - C) Generating text
   - D) None of the above
   - **Answer**: B*

4. What does 'k-shot prompting' entail?
   - A) Providing no examples
   - B) Providing multiple examples
   - C) Providing one example
   - D) Providing random inputs
   - **Answer**: B*

5. What is a primary method to mitigate hallucinations in LLMs?
   - A) Fine-tuning
   - B) RAG
   - C) Greedy decoding
   - D) Soft prompting
   - **Answer**: B*

6. Which of the following is NOT a decoding strategy?
   - A) Beam Search
   - B) Greedy Decoding
   - C) Temperature Adjustment
   - D) Chain-of-Thought
   - **Answer**: D*

### Short Answer
1. Define `hallucination` in the context of LLMs.
   - **Answer**: Hallucination refers to text generated by LLMs that is not grounded in training data, often resulting in factually incorrect but fluent-sounding content.

2. What is the purpose of RAG systems?
   - **Answer**: RAG systems provide external documents as context to LLMs to reduce hallucination and improve the accuracy of generated responses.

3. Explain the difference between fine-tuning and parameter-efficient methods like LoRA.
   - **Answer**: Fine-tuning adjusts all model parameters for a specific task, while parameter-efficient methods like LoRA modify only a subset of parameters, reducing computational costs.

4. Describe `soft prompting`.
   - **Answer**: Soft prompting is a training method that adds learnable parameters to prompts, which are fine-tuned to cue specific tasks during model training.

5. What is a key challenge with prompt engineering?
   - **Answer**: The key challenge is that it can be unintuitive and requires iterative refinement to achieve desired outputs, making the process complex.

6. What are language agents in the context of LLMs?
   - **Answer**: Language agents are models designed to operate in environments and take sequential actions to accomplish specific goals based on their interactions.

### Long Form Questions
1. Discuss the impact of temperature in decoding strategies and its influence on text generation.
   - **Marking Rubric**:
     - Definition of temperature (1 point)
     - Explanation of its role in controlling output creativity vs conservativism (2 points)
     - Examples of how different temperatures affect output (2 points)

2. Analyze the limitations of current code models and potential future advancements in LLM technology.
   - **Marking Rubric**:
     - Description of current limitations, particularly in bug fixing (2 points)
     - Discussion on multi-modal capabilities and their importance (2 points)
     - Speculation on future advancements and applications of LLMs (2 points)

**Answer Key**:
- MC: 1-A, 2-B, 3-B, 4-B, 5-B, 6-D
- SA: (Answers as specified)
- Long Form: (Grading based on rubric)
```
